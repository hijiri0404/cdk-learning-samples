"""
ğŸŒ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰API Lambdaé–¢æ•°

ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ»ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»ç®¡ç†ã®ãŸã‚ã®REST API ã‚’æä¾›ã—ã¾ã™ã€‚
ç½²åä»˜ãURLã‚’ä½¿ç”¨ã—ã¦ã‚»ã‚­ãƒ¥ã‚¢ãªãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œã‚’å®Ÿç¾ã—ã¾ã™ã€‚
"""

import json
import boto3
import uuid
import os
from datetime import datetime, timedelta
from typing import Dict, Any
import logging

# ãƒ­ã‚°è¨­å®š
logger = logging.getLogger()
logger.setLevel(logging.INFO)

# AWS ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
s3_client = boto3.client('s3')

# ç’°å¢ƒå¤‰æ•°å–å¾—
UPLOAD_BUCKET = os.environ['UPLOAD_BUCKET']
PROCESSED_BUCKET = os.environ['PROCESSED_BUCKET']
MAX_FILE_SIZE_MB = int(os.environ.get('MAX_FILE_SIZE_MB', '10'))
ALLOWED_FILE_TYPES = os.environ.get('ALLOWED_FILE_TYPES', '.jpg,.png,.pdf').split(',')


def lambda_handler(event: Dict[str, Any], context: Any) -> Dict[str, Any]:
    """
    API Gateway ã‹ã‚‰ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å‡¦ç†
    """
    try:
        http_method = event.get('httpMethod', 'GET')
        path = event.get('path', '/')
        path_parameters = event.get('pathParameters') or {}
        body = event.get('body')
        
        if body:
            body = json.loads(body)
        
        logger.info(f"API Request: {http_method} {path}")
        
        # ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
        if path == '/upload' and http_method == 'POST':
            return get_upload_url(body)
        elif path == '/files' and http_method == 'GET':
            return list_files()
        elif path.startswith('/files/') and http_method == 'GET':
            file_id = path_parameters.get('fileId')
            return get_file_info(file_id)
        elif path.startswith('/files/') and http_method == 'DELETE':
            file_id = path_parameters.get('fileId')
            return delete_file(file_id)
        elif path.startswith('/download/') and http_method == 'GET':
            file_id = path_parameters.get('fileId')
            return get_download_url(file_id)
        elif path.startswith('/status/') and http_method == 'GET':
            file_id = path_parameters.get('fileId')
            return get_processing_status(file_id)
        else:
            return create_response(404, {'error': 'Endpoint not found'})
            
    except Exception as e:
        logger.error(f"API Error: {str(e)}")
        return create_response(500, {'error': 'Internal server error'})


def create_response(status_code: int, body: Dict[str, Any]) -> Dict[str, Any]:
    """
    API Gateway ãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼ã‚’ä½œæˆ
    """
    return {
        'statusCode': status_code,
        'headers': {
            'Content-Type': 'application/json',
            'Access-Control-Allow-Origin': '*',
            'Access-Control-Allow-Methods': 'GET,POST,PUT,DELETE,OPTIONS',
            'Access-Control-Allow-Headers': 'Content-Type,X-Api-Key'
        },
        'body': json.dumps(body, ensure_ascii=False, indent=2)
    }


def get_upload_url(body: Dict[str, Any]) -> Dict[str, Any]:
    """
    ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ç”¨ã®ç½²åä»˜ãURL ã‚’ç”Ÿæˆ
    """
    try:
        if not body:
            return create_response(400, {'error': 'Request body required'})
        
        filename = body.get('filename')
        content_type = body.get('content_type', 'application/octet-stream')
        file_size = body.get('file_size', 0)
        
        if not filename:
            return create_response(400, {'error': 'filename is required'})
        
        # ãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼
        if not validate_upload_request(filename, file_size):
            return create_response(400, {'error': 'Invalid file'})
        
        # ä¸€æ„ãªãƒ•ã‚¡ã‚¤ãƒ«IDã¨ã‚­ãƒ¼ã‚’ç”Ÿæˆ
        file_id = str(uuid.uuid4())
        file_key = f"uploads/{file_id}/{filename}"
        
        # ç½²åä»˜ãURLç”Ÿæˆï¼ˆ15åˆ†æœ‰åŠ¹ï¼‰
        presigned_url = s3_client.generate_presigned_url(
            'put_object',
            Params={
                'Bucket': UPLOAD_BUCKET,
                'Key': file_key,
                'ContentType': content_type,
                'Metadata': {
                    'file-id': file_id,
                    'original-filename': filename,
                    'upload-time': datetime.now().isoformat()
                }
            },
            ExpiresIn=900  # 15åˆ†
        )
        
        return create_response(200, {
            'upload_url': presigned_url,
            'file_id': file_id,
            'file_key': file_key,
            'expires_in': 900,
            'instructions': {
                'method': 'PUT',
                'headers': {
                    'Content-Type': content_type
                }
            }
        })
        
    except Exception as e:
        logger.error(f"Error generating upload URL: {str(e)}")
        return create_response(500, {'error': 'Failed to generate upload URL'})


def validate_upload_request(filename: str, file_size: int) -> bool:
    """
    ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®æ¤œè¨¼
    """
    # ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­ãƒã‚§ãƒƒã‚¯
    file_extension = os.path.splitext(filename)[1].lower()
    if file_extension not in ALLOWED_FILE_TYPES:
        logger.warning(f"Invalid file type: {file_extension}")
        return False
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
    max_size_bytes = MAX_FILE_SIZE_MB * 1024 * 1024
    if file_size > max_size_bytes:
        logger.warning(f"File too large: {file_size} bytes")
        return False
    
    return True


def list_files() -> Dict[str, Any]:
    """
    ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸€è¦§ã‚’å–å¾—
    """
    try:
        files = []
        
        # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒã‚±ãƒƒãƒˆã®ãƒ•ã‚¡ã‚¤ãƒ«
        paginator = s3_client.get_paginator('list_objects_v2')
        for page in paginator.paginate(Bucket=UPLOAD_BUCKET, Prefix='uploads/'):
            for obj in page.get('Contents', []):
                files.append({
                    'file_id': obj['Key'].split('/')[1],
                    'key': obj['Key'],
                    'size': obj['Size'],
                    'last_modified': obj['LastModified'].isoformat(),
                    'status': 'uploaded',
                    'bucket': 'upload'
                })
        
        # å‡¦ç†æ¸ˆã¿ãƒã‚±ãƒƒãƒˆã®ãƒ•ã‚¡ã‚¤ãƒ«
        for page in paginator.paginate(Bucket=PROCESSED_BUCKET, Prefix='processed/'):
            for obj in page.get('Contents', []):
                files.append({
                    'file_id': obj['Key'].split('/')[2] if len(obj['Key'].split('/')) > 2 else 'unknown',
                    'key': obj['Key'],
                    'size': obj['Size'],
                    'last_modified': obj['LastModified'].isoformat(),
                    'status': 'processed',
                    'bucket': 'processed'
                })
        
        return create_response(200, {
            'files': files,
            'count': len(files)
        })
        
    except Exception as e:
        logger.error(f"Error listing files: {str(e)}")
        return create_response(500, {'error': 'Failed to list files'})


def get_file_info(file_id: str) -> Dict[str, Any]:
    """
    ç‰¹å®šãƒ•ã‚¡ã‚¤ãƒ«ã®æƒ…å ±ã‚’å–å¾—
    """
    try:
        if not file_id:
            return create_response(400, {'error': 'file_id is required'})
        
        # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒã‚±ãƒƒãƒˆã§æ¤œç´¢
        try:
            objects = s3_client.list_objects_v2(
                Bucket=UPLOAD_BUCKET,
                Prefix=f'uploads/{file_id}/'
            )
            
            if 'Contents' in objects:
                obj = objects['Contents'][0]
                metadata = s3_client.head_object(Bucket=UPLOAD_BUCKET, Key=obj['Key'])
                
                return create_response(200, {
                    'file_id': file_id,
                    'key': obj['Key'],
                    'size': obj['Size'],
                    'last_modified': obj['LastModified'].isoformat(),
                    'content_type': metadata.get('ContentType'),
                    'metadata': metadata.get('Metadata', {}),
                    'status': 'uploaded',
                    'bucket': 'upload'
                })
        except:
            pass
        
        # å‡¦ç†æ¸ˆã¿ãƒã‚±ãƒƒãƒˆã§æ¤œç´¢
        try:
            objects = s3_client.list_objects_v2(
                Bucket=PROCESSED_BUCKET,
                Prefix=f'processed/uploads/{file_id}/'
            )
            
            if 'Contents' in objects:
                obj = objects['Contents'][0]
                metadata = s3_client.head_object(Bucket=PROCESSED_BUCKET, Key=obj['Key'])
                
                return create_response(200, {
                    'file_id': file_id,
                    'key': obj['Key'],
                    'size': obj['Size'],
                    'last_modified': obj['LastModified'].isoformat(),
                    'content_type': metadata.get('ContentType'),
                    'metadata': metadata.get('Metadata', {}),
                    'status': 'processed',
                    'bucket': 'processed'
                })
        except:
            pass
        
        return create_response(404, {'error': f'File {file_id} not found'})
        
    except Exception as e:
        logger.error(f"Error getting file info: {str(e)}")
        return create_response(500, {'error': 'Failed to get file info'})


def get_download_url(file_id: str) -> Dict[str, Any]:
    """
    ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç”¨ã®ç½²åä»˜ãURL ã‚’ç”Ÿæˆ
    """
    try:
        if not file_id:
            return create_response(400, {'error': 'file_id is required'})
        
        # å‡¦ç†æ¸ˆã¿ãƒã‚±ãƒƒãƒˆã§æ¤œç´¢
        objects = s3_client.list_objects_v2(
            Bucket=PROCESSED_BUCKET,
            Prefix=f'processed/uploads/{file_id}/'
        )
        
        if 'Contents' not in objects:
            return create_response(404, {'error': 'File not found or not processed yet'})
        
        file_key = objects['Contents'][0]['Key']
        
        # ç½²åä»˜ãURLç”Ÿæˆï¼ˆ1æ™‚é–“æœ‰åŠ¹ï¼‰
        download_url = s3_client.generate_presigned_url(
            'get_object',
            Params={
                'Bucket': PROCESSED_BUCKET,
                'Key': file_key
            },
            ExpiresIn=3600  # 1æ™‚é–“
        )
        
        return create_response(200, {
            'download_url': download_url,
            'file_id': file_id,
            'expires_in': 3600
        })
        
    except Exception as e:
        logger.error(f"Error generating download URL: {str(e)}")
        return create_response(500, {'error': 'Failed to generate download URL'})


def delete_file(file_id: str) -> Dict[str, Any]:
    """
    ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
    """
    try:
        if not file_id:
            return create_response(400, {'error': 'file_id is required'})
        
        deleted_files = []
        
        # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒã‚±ãƒƒãƒˆã‹ã‚‰å‰Šé™¤
        try:
            objects = s3_client.list_objects_v2(
                Bucket=UPLOAD_BUCKET,
                Prefix=f'uploads/{file_id}/'
            )
            
            for obj in objects.get('Contents', []):
                s3_client.delete_object(Bucket=UPLOAD_BUCKET, Key=obj['Key'])
                deleted_files.append(f"upload:{obj['Key']}")
        except:
            pass
        
        # å‡¦ç†æ¸ˆã¿ãƒã‚±ãƒƒãƒˆã‹ã‚‰å‰Šé™¤
        try:
            objects = s3_client.list_objects_v2(
                Bucket=PROCESSED_BUCKET,
                Prefix=f'processed/uploads/{file_id}/'
            )
            
            for obj in objects.get('Contents', []):
                s3_client.delete_object(Bucket=PROCESSED_BUCKET, Key=obj['Key'])
                deleted_files.append(f"processed:{obj['Key']}")
        except:
            pass
        
        if not deleted_files:
            return create_response(404, {'error': 'File not found'})
        
        return create_response(200, {
            'message': f'File {file_id} deleted successfully',
            'deleted_files': deleted_files
        })
        
    except Exception as e:
        logger.error(f"Error deleting file: {str(e)}")
        return create_response(500, {'error': 'Failed to delete file'})


def get_processing_status(file_id: str) -> Dict[str, Any]:
    """
    ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†çŠ¶æ³ã‚’ç¢ºèª
    """
    try:
        if not file_id:
            return create_response(400, {'error': 'file_id is required'})
        
        # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒã‚±ãƒƒãƒˆã§ç¢ºèªï¼ˆæœªå‡¦ç†ï¼‰
        upload_objects = s3_client.list_objects_v2(
            Bucket=UPLOAD_BUCKET,
            Prefix=f'uploads/{file_id}/'
        )
        
        # å‡¦ç†æ¸ˆã¿ãƒã‚±ãƒƒãƒˆã§ç¢ºèªï¼ˆå‡¦ç†æ¸ˆã¿ï¼‰
        processed_objects = s3_client.list_objects_v2(
            Bucket=PROCESSED_BUCKET,
            Prefix=f'processed/uploads/{file_id}/'
        )
        
        if 'Contents' in processed_objects:
            status = 'completed'
            message = 'File processing completed'
        elif 'Contents' in upload_objects:
            status = 'processing'
            message = 'File is being processed'
        else:
            status = 'not_found'
            message = 'File not found'
        
        return create_response(200, {
            'file_id': file_id,
            'status': status,
            'message': message
        })
        
    except Exception as e:
        logger.error(f"Error checking processing status: {str(e)}")
        return create_response(500, {'error': 'Failed to check processing status'})